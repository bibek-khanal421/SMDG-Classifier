{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    "import tensorflow as tf \n",
    "import tensorflow_addons as tfa\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from models import Xception, ResNet50\n",
    "sns.set_style(\"darkgrid\")\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('archive\\metadata - standardized.csv')\n",
    "df = pd.DataFrame(df[['names','types']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['types'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['types'].value_counts())\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "sns.countplot(df['types'].value_counts(), x=df['types'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = r'archive\\full-fundus\\full-fundus'\n",
    "image_size = 256\n",
    "seed = 12\n",
    "batch_size = 64\n",
    "\n",
    "def get_path(name):\n",
    "    return image_directory + '\\\\' + name + '.png'\n",
    "\n",
    "df['path'] = df['names'].apply(get_path)\n",
    "df.drop('names', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data = train_test_split(df, test_size=0.2, random_state=seed)\n",
    "train_data, test_data = train_test_split(train_data, test_size=0.1, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom Image data generator class from Image paths and labels\n",
    "    \"\"\"\n",
    "    def __init__(self, data: pd.DataFrame, batch_size: int=batch_size, image_size: tuple=image_size, balance: bool=False, augment: bool=False):\n",
    "        if balance:\n",
    "            self.data = self.__balance__(data)\n",
    "        else:\n",
    "            self.data = data\n",
    "        self.labels = self.data['types'].values\n",
    "        self.image_path = self.data['path'].values\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = (image_size, image_size)\n",
    "        self.augment = augment\n",
    "        self.augment_pipe = tf.keras.Sequential([\n",
    "            tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomZoom(0.3),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "            tf.keras.layers.experimental.preprocessing.RandomTranslation(0.2, 0.2)\n",
    "        ])\n",
    "    \n",
    "    def __augment__(self, image):\n",
    "        if np.random.rand() < 0:\n",
    "            image = self.augment_pipe(image)\n",
    "        return image\n",
    "\n",
    "    def __balance__(self, data):\n",
    "        data_pos, data_neg = data[data['types'] == 1], data[data['types'] == 0]\n",
    "        data_neg = data_neg.sample(len(data_pos), replace=False, random_state=seed)\n",
    "        data = pd.concat([data_pos, data_neg])\n",
    "        data = data.sample(frac=1)\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.math.ceil(len(self.image_path) / self.batch_size)\n",
    "\n",
    "    def __get_image__(self, image_path):\n",
    "        image = tf.keras.preprocessing.image.load_img(image_path, target_size=self.image_size)\n",
    "        image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        return image\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        batch_images = self.image_path[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        batch_images = np.array([self.__augment__(self.__get_image__(path)) for path in batch_images]) / 255.0\n",
    "        batch_labels = self.labels[index * self.batch_size : (index + 1) * self.batch_size]\n",
    "        return batch_images, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageGenerator(train_data, balance=True, augment=True)\n",
    "val_dataset = ImageGenerator(val_data, balance=False, augment=False)\n",
    "test_dataset = ImageGenerator(test_data, balance=False, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "im,l = train_dataset.__getitem__(np.random.randint(0, len(train_dataset)))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i+1)\n",
    "  plt.imshow(im[i])\n",
    "  plt.title(l[i])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50 Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "weight_decay = 0.0006\n",
    "beta = 0.5\n",
    "num_epochs = 50\n",
    "checkpoint_filepath_res = r'F:\\Glaucoma\\saved_model\\resnet\\weights.h5'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#     project=\"Glaucoma Detection\",\n",
    "#     name = 'ResNet50',\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"Dataset\": \"Glaucoma\",\n",
    "#         \"Models\": \"ResNet50\",\n",
    "#         \"learning_rate\": learning_rate,\n",
    "#         \"weight_decay\": weight_decay,\n",
    "#         \"metrics\": [\"accuracy\"],\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, load_checkpoint=True):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, \n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "           'accuracy',\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath_res,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "\n",
    "    if load_checkpoint:\n",
    "        model.load_weights(checkpoint_filepath_res)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[checkpoint_callback,\n",
    "                   early_stopping_callback,\n",
    "                #    WandbMetricsLogger(log_freq='batch')\n",
    "                   ],\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evalutation(model, dataset):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, \n",
    "        weight_decay=weight_decay, \n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "        metrics=[\n",
    "           'accuracy',\n",
    "        ],\n",
    "    )\n",
    "    model.load_weights(checkpoint_filepath_res)\n",
    "    _, accuracy = model.evaluate(dataset)\n",
    "    predictions = model.predict(dataset)\n",
    "    predictions = tf.where(predictions < 0.5, 0, 1)\n",
    "    report  = classification_report(dataset.labels, predictions)\n",
    "    return accuracy, report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "res_classifier = ResNet50(input_shape=(image_size, image_size, 3), num_classes=1).build_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run_training(res_classifier, load_checkpoint=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, report = run_evalutation(res_classifier, test_dataset)\n",
    "pred_label = res_classifier.predict(test_dataset) > 0.5\n",
    "precision,recall,f1_score,_ =  precision_recall_fscore_support(test_dataset.labels, pred_label, average='macro')\n",
    "cm = confusion_matrix(test_dataset.labels, pred_label)\n",
    "print(\"Accuracy on test dataset: \", accuracy)\n",
    "print(\"Classification report:\\n\", report)\n",
    "print(\"Confusion matrix:\\n\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Glaucoma', 'Normal'])\n",
    "disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cam import GRADgen\n",
    "res_classifier.load_weights(checkpoint_filepath_res)\n",
    "for i in range(3):\n",
    "    random_batch = np.random.randint(0, 10)\n",
    "    random_index = np.random.randint(0, 32)\n",
    "    output, image, preds = GRADgen(res_classifier, 'conv5_block3_out', test_dataset.__getitem__(random_batch)[0][random_index])\n",
    "    print(f\"Predicted label: {preds} \\nActual label: {test_dataset.__getitem__(random_batch)[1][random_index]}\")\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(3, 2, 2*i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(3, 2, 2*i+2)\n",
    "    plt.imshow(output)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
